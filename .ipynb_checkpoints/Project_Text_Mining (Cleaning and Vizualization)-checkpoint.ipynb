{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03298db0",
   "metadata": {},
   "source": [
    "# Project Text Mining (Part 1) \n",
    "\n",
    "This notebook was made as the Text Mining Project for Master Data Mining of Université Lyon 2. \n",
    "\n",
    "## Objective:\n",
    "\n",
    "The main goal is to pratice the concepts seen in class and apply text mining tecniques in a dataset. \n",
    "\n",
    "## Plan:\n",
    "\n",
    "This notebook will be separated into the following sections\n",
    "\n",
    "$\\rightarrow$ Acquisition des données \n",
    "\n",
    "$\\rightarrow$. Construction d’un index sur les mots\n",
    "\n",
    "$\\rightarrow$. Regrouper les documents par cluster et/ou thématique\n",
    "\n",
    "\n",
    "**Owners**: Lia Furtado and Hugo Vinision \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad5627",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d82e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5137c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "import seaborn as sns\n",
    "import math\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import Isomap\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a713a",
   "metadata": {},
   "source": [
    "---\n",
    "## Acquisition des données\n",
    "\n",
    "This part consists in loading the dataset from https://www.aminer.org/citation and creating a usable dataset with smaller size and better structure. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e520815d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dblp-ref/dblp-ref-0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Loading the json dataset retrieved from the website \u001b[39;00m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdblp-ref/dblp-ref-0.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dblp-ref/dblp-ref-0.json'"
     ]
    }
   ],
   "source": [
    "#Loading the json dataset retrieved from the website \n",
    "data = []\n",
    "with open('dblp-ref/dblp-ref-0.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "#Get a small subset of data by filtering only the citations from 2016\n",
    "\n",
    "data_2016 = df[(df['year'] == 2016)]\n",
    "\n",
    "#Remove movies with abstract null\n",
    "data_2016 = data_2016[~data_2016['abstract'].isnull()] \n",
    "#Reindex the dataframe\n",
    "data_2016.reset_index(drop=True, inplace=True)\n",
    "data = data_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f60125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059d5bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Construction d’un index sur les mots\n",
    "\n",
    "\n",
    "This section is to first preprocess the texts by removing stop-words, frequent words, rare words, perform stemming or lemmatization etc. Then to vizualize some common words and understand the data. Finally, to vectorize the words by performing Count Vectorizer or TF-IDF transformation. This two methods are explained more bellow:\n",
    "\n",
    "\n",
    "* Text Cleaning and pre-processing Authors and Conferences information\n",
    "* Data Exploratory analysis (by visualization)\n",
    "* Text Vectorization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21028e39",
   "metadata": {},
   "source": [
    "### Text Cleaning and pre-processing Authors and Conferences information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f28907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining the title and abstract text\n",
    "data['text'] = data['title'] + ' ' + data['abstract']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_en = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_authors(msg):\n",
    "    #removing []\n",
    "    sentence = re.sub(r\"[\\([{})\\]]\",r'', msg)\n",
    "    #removing '' and .\n",
    "    sentence = sentence.replace(\"'\", \"\").replace(\".\", \"\") \n",
    "    sentence = sentence.replace(\"\\\\\", \"\").replace(\"/\", \"\") \n",
    "    sentence = sentence.replace('\"','') \n",
    "    #remove all non latin caracters\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]',r'', sentence)\n",
    "    #removing digits\n",
    "    sentence = re.sub(\"\\S*\\d+\\S*\", \"\", sentence)\n",
    "    #remove diactric accents and greek letters\n",
    "    sentence = ''.join(c for c in unicodedata.normalize('NFD', sentence)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "    #hyphen the authors\n",
    "    sentence = list(sentence.split(\", \"))\n",
    "    sentence = [word.lower().replace(' ','-') for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a13c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_venue(msg):\n",
    "    #removing pontuation\n",
    "    No_Punctuation = [char if char not in string.punctuation else ' ' for char in msg ]\n",
    "    sentence = ''.join(No_Punctuation)\n",
    "    #removing []\n",
    "    sentence = re.sub(r\"[\\([{})\\]]\",r'', msg)\n",
    "    #removing '' and .\n",
    "    sentence = sentence.replace(\":\", \"\") \n",
    "    sentence = sentence.replace(\".\", \"\")\n",
    "    sentence = sentence.replace(\"\\\\\", \"\").replace(\"/\", \"\") \n",
    "    sentence = sentence.replace('\"','') \n",
    "    sentence = sentence.replace(\"& \", \"\")\n",
    "    sentence = sentence.replace(\"and \", \"\")\n",
    "    #hyphen the authors\n",
    "    sentence = list(sentence.split(\", \"))\n",
    "    sentence = [word.lower().replace(' ','-') for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(msg):\n",
    "    #removing pontuation\n",
    "    No_Punctuation = [char if char not in string.punctuation else ' ' for char in msg ]\n",
    "    sentence = ''.join(No_Punctuation)\n",
    "    #remove all non latin caracters\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]',r'', sentence)\n",
    "    #removing digits\n",
    "    sentence = re.sub(\"\\S*\\d+\\S*\", \"\", sentence)\n",
    "    #remove diactric accents and greek letters\n",
    "    sentence = ''.join(c for c in unicodedata.normalize('NFD', sentence)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "    #### Word tokenization is the process of splitting up “sentences” into “words”\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    #Stemming the words\n",
    "    #stemmer = PorterStemmer()\n",
    "    return \" \".join(word.lower() for word in sentence if word.lower() not in stopwords_en and len(word.lower())>1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most 40 used words in bag of words\n",
    "def clean_words(msg):\n",
    "    erase_words = ['based','data', 'proposed', 'paper', 'model','method','results','time','algorithm','using','problem', \\\n",
    "                   'two', 'system','performance','approach','network','show','also','information','analysis','new', \\\n",
    "                   'used','systems', 'different','study','methods','networks','number','one','order','set','algorithms',\\\n",
    "                   'high','control','models','propose','learning','use','image','problems']\n",
    "    \n",
    "    return \" \".join(char for char in word_tokenize(msg) if char not in erase_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d006151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6768737",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].apply(lambda x:cleanup_text(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x:clean_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e57e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean_tokenized'] = data['text_clean'].apply(lambda x:nltk.word_tokenize(x))\n",
    "data = data[~data['venue'].isnull()] \n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc13f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['authors_clean'] = data['authors'].apply(lambda x:cleanup_authors(str(x)))\n",
    "data['venue_clean'] = data['venue'].apply(lambda x:cleanup_venue(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f4639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dblp_2016_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe767607",
   "metadata": {},
   "source": [
    "### Data Exploratory analysis (by visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed3ed",
   "metadata": {},
   "source": [
    "**Most Common words** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb26b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in list(data['text_clean'])]\n",
    "bag_of_words = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "#Most common words\n",
    "counter = Counter(bag_of_words)\n",
    "most = counter.most_common()\n",
    "\n",
    "for word,count in most[:30]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.xticks(fontsize=18, rotation=90)\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c696b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating the wordcloud with the values under the category dataframe\n",
    "plt.figure(figsize=(10,8))\n",
    "word_cloud = WordCloud(background_color='black',\n",
    "                          max_font_size = 80\n",
    "                         ).generate(\" \".join(bag_of_words))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9e23e",
   "metadata": {},
   "source": [
    "**Most Common Authors and Venues** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_authors = [item for sublist in data['authors_clean'] for item in sublist]\n",
    "\n",
    "bag_of_venues = [item for sublist in data['venue_clean'] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 authors\n",
    "plt.figure(figsize=(8, 10)) \n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "top10authors = pd.DataFrame.from_records(\n",
    "    Counter(bag_of_authors).most_common(10), columns=[\"Author\", \"Count\"]\n",
    ")\n",
    "sns.barplot(x=\"Count\", y=\"Author\", data=top10authors, palette=\"RdBu_r\")\n",
    "plt.title(\"Top 10 Authors\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# TOP 10 Venues\n",
    "top10venues = pd.DataFrame.from_records(\n",
    "    Counter(bag_of_venues).most_common(10),\n",
    "    columns=[\"Venues\", \"Count\"],\n",
    ")\n",
    "\n",
    "sns.barplot(x=\"Count\", y=\"Venues\", data=top10venues)\n",
    "plt.title(\"Top 10 Venues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc946f",
   "metadata": {},
   "source": [
    "**Network  Vizualization of the top 20 authors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae05490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_connections = data[['title', 'text_clean', 'text_clean_tokenized','authors_clean', 'venue_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a29f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20authors = pd.DataFrame.from_records(\n",
    "    Counter(bag_of_authors).most_common(20), columns=[\"Name\", \"Count\"])\n",
    "\n",
    "articles_20_common_authors = []\n",
    "for index, connection in df_connections.iterrows():\n",
    "    for author in list(top20authors['Name']):\n",
    "        if (author in connection['authors_clean']):\n",
    "            articles_20_common_authors.append(connection)\n",
    "            \n",
    "df_20 = pd.DataFrame(articles_20_common_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd22762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_20['authors_combination'] = df_20['authors_clean'].apply(lambda x: list(combinations(x[::-1], 2)))\n",
    "df_20 = df_20.explode('authors_combination','venue_clean')\n",
    "\n",
    "df_20 = df_20[~df_20['authors_combination'].isnull()] \n",
    "df_20.reset_index(inplace=True, drop=True)\n",
    "df_20['From'], df_20['To'] = zip(*df_20.authors_combination)\n",
    "\n",
    "df_graph = df_20[['From', 'To', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MG= nx.from_pandas_edgelist(df_graph, 'From', 'To', edge_attr=['title'], \n",
    "                                 create_using=nx.MultiGraph())\n",
    "\n",
    "MG.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(height='650px', width='100%', font_color='black', notebook =True)\n",
    "net.from_nx(MG)\n",
    "net.show(\"example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13736057",
   "metadata": {},
   "source": [
    "### Text Vectorization\n",
    "\n",
    "$\\rightarrow$ **Term Frequency Inverse Document Frequency Vectorizer**\n",
    "```\n",
    "This vectorizer considers in the overall documents the weight of words.\n",
    "\n",
    "Tfidf is equals to number of word appears in a document times the inverse document frequency of the word across the set of \n",
    "documents  \n",
    "```\n",
    "\n",
    "$\\rightarrow$ **Doc2Vec**\n",
    "\n",
    "\n",
    "$\\rightarrow$ **BERT**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418e687",
   "metadata": {},
   "source": [
    "**TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed429e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "vectors = vectorizer.fit_transform(data['text_clean'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_vectors = vectors.todense()\n",
    "\n",
    "tfid = pd.DataFrame(tfidf_vectors, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4647e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualizer = FreqDistVisualizer(features=feature_names, orient='v', size=(1080, 560))\n",
    "visualizer.fit(vectors)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2353409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bb5bf",
   "metadata": {},
   "source": [
    "**Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = []\n",
    "\n",
    "for i, list_tokens in enumerate(data['text_clean_tokenized']):\n",
    "    tagged_docs.append(TaggedDocument(words=list_tokens, tags=[str(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Doc2Vec...')\n",
    "\n",
    "d2v_model = Doc2Vec(vector_size=100, window=10, min_count=5, workers=11,alpha=0.025)\n",
    "d2v_model.build_vocab(tagged_docs)\n",
    "d2v_model.train(tagged_docs,total_examples=d2v_model.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9945b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse documents and print some info\n",
    "print('Parsing documents...')\n",
    "\n",
    "doc2vec_vectors = []\n",
    "\n",
    "for index, list_tokens in enumerate(data['text_clean_tokenized']):\n",
    "    doc2vec_vectors.append(d2v_model.dv[index])\n",
    "        \n",
    "doc2vec_vectors = np.array(doc2vec_vectors)\n",
    "\n",
    "print('Total number of documents parsed: {}'.format(len(doc2vec_vectors)))\n",
    "print('Size of vector embeddings: ', doc2vec_vectors.shape[1])\n",
    "print('Shape of vectors embeddings matrix: ', doc2vec_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802d061",
   "metadata": {},
   "source": [
    "---\n",
    "## Regrouper les documents par cluster et/ou thématique\n",
    "\n",
    "\n",
    "This section is to first preprocess the texts by removing stop-words, frequent words, rare words, perform stemming or lemmatization etc. Then to vizualize some common words and understand the data. Finally, to vectorize the words by performing Count Vectorizer or TF-IDF transformation. This two methods are explained more bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19dbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = 10\n",
    "representations = {\"Doc2Vec\" : doc2vec_vectors, \\\n",
    "                   \"TF-IDF\" : np.asarray(tfidf_vectors)}\n",
    "\n",
    "print('Clustering documents...')\n",
    "\n",
    "results = []\n",
    "for key, value in representations.items():\n",
    "    print(\"Representation \" + str(key))\n",
    "    labels = KMeans(n_clusters=k, random_state=0).fit_predict(value)\n",
    "    results.append({\"Representation\" : key, \"labels\" :list(labels)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc644a20",
   "metadata": {},
   "source": [
    "**Clustering vizualization in each embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893db029",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Doc2Vec\", \"Word2Vec\", \"TF-IDF\"))\n",
    "i = 1\n",
    "for res in results:\n",
    "    \n",
    "    print('Isomap for ' + str(res['Representation']) +'...')\n",
    "\n",
    "    #embedding = TSNE(n_components=2, init='pca')\n",
    "    embedding = Isomap(n_components=2)\n",
    "\n",
    "    components = embedding.fit_transform(representations[res['Representation']])\n",
    "    labels = res['labels']\n",
    "    \n",
    "    print('Plot for ' + str(res['Representation']))\n",
    "\n",
    "    X = components[:, 0]\n",
    "    y = components[:, 1]\n",
    "    \n",
    "    fig1 = px.scatter(X,y,color=labels)\n",
    "    trace1 = fig1['data'][0]\n",
    "\n",
    "    fig.add_trace(trace1, row=i, col=1)\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "fig.update_layout(height=1500, width=600, title_text=\"Embedding x Vizu\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec74f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_lda = umap.UMAP(metric='cosine').fit_transform(lda_vectors)\n",
    "mapper_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    mapper_lda[:, 0],\n",
    "    mapper_lda[:, 1])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Penguin dataset', fontsize=24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
